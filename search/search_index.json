{"docs":[{"location":"/paradox.json","text":"","title":""},{"location":"/docs/index.html","text":"","title":"Blue Brain Nexus"},{"location":"/docs/index.html#blue-brain-nexus","text":"The BlueBrain Nexus is a provenance based, semantic enabled data management platform enabling the definition of an arbitrary domain of application for which there is a need to create and manage entities as well as their relations (e.g. provenance). For example, the domain of application managed by the Nexus platform deployed at Blue Brain is to digitally reconstruct and simulate the brain.\nAt the heart of the BlueBrain Nexus platform lies the Knowledge Graph; at BlueBrain, it will allow scientists to:\nRegister and manage neuroscience relevant entity types through schemas that can reuse or extend community defined schemas (e.g. schema.org, bioschema.org, W3C-PROV) and ontologies (e.g. brain parcellation schemes, cell types, taxonomy). Submit data to the platform and describe their provenance using the W3C PROV model. Provenance is about how data or things are generated (e.g. protocols, methods used…), when (e.g. timeline) and by whom (e.g. people, software…). Provenance supports the data reliability and quality assessment as well as enables workflow reproducibility. Platform users can submit data either through web forms or programmatic interfaces. Search, discover, reuse and derive high-quality neuroscience data generated within and outside the platform for the purpose of driving their own scientific endeavours. Data can be examined by species, contributing laboratory, methodology, brain region, and data type, thereby allowing functionality not currently available elsewhere. The data are predominantly organized into atlases (e.g. Allen CCF, Waxholm) and linked to the KnowledgeSpace – a collaborative community-based encyclopedia linking brain research concepts to the latest data, models and literature.\nIt is to be noted that many other scientific fields (Astronomy, Agriculture, Bioinformatics, Pharmaceutical Industry, …) are in need of such a technology. Consequently, BlueBrain Nexus core technology is being developed to be agnostic of the domain it might be applied to.","title":"Blue Brain Nexus"},{"location":"/docs/index.html#nexus-components","text":"The Nexus platform is made up of a collection of services and web applications that work together to manage data stored within the system. The services and web applications are powered by a collection of libraries and tools built specifically to address the needs of the platform. Underneath it all there are popular open source technologies that we all know and love.","title":"Nexus Components"},{"location":"/docs/index.html#nexus-services","text":"","title":"Nexus Services"},{"location":"/docs/index.html#nexus-knowledgegraph","text":"This service is the heart of the BlueBrain Nexus platform. It allows users to define their domain, populate the knowledge graph with data, attach files to data. It also provides semantic search facilities to discover similar and relevant data in the platform.\nSource Code | Documentation","title":"Nexus KnowledgeGraph"},{"location":"/docs/index.html#nexus-admin","text":"This service manages the platform wide scopes for data and their configuration (i.e.: the API mapping).\nSource Code | Documentation","title":"Nexus Admin"},{"location":"/docs/index.html#nexus-iam","text":"This service manages the access to data within the platform. It makes use of configurable downstream OpenID Connect compliant identity providers to authenticate clients and manages the access controls for the entire platform.\nSource Code | Documentation","title":"Nexus IAM"},{"location":"/docs/index.html#nexus-web-applications","text":"","title":"Nexus Web Applications"},{"location":"/docs/index.html#nexus-search","text":"This web application allows users of the nexus platform to search in the knowledge graph. Beyond searching and inspecting data stored in the platform, its purpose is to enable the discovery of similar and related data.\nSource Code","title":"Nexus Search"},{"location":"/docs/index.html#nexus-explorer","text":"This web application allows users to browse the data within the system.\nSource Code","title":"Nexus Explorer"},{"location":"/docs/index.html#nexus-docs","text":"Generated documentation for the platform (this website).\nSource Code | Website","title":"Nexus Docs"},{"location":"/docs/getting-started/index.html","text":"","title":"Getting Started Guide"},{"location":"/docs/getting-started/index.html#getting-started-guide","text":"","title":"Getting Started Guide"},{"location":"/docs/getting-started/running-nexus/index.html","text":"","title":"Running Nexus"},{"location":"/docs/getting-started/running-nexus/index.html#running-nexus","text":"TBC.","title":"Running Nexus"},{"location":"/docs/getting-started/running-nexus/index.html#using-the-public-sandbox","text":"TBC.","title":"Using the public sandbox"},{"location":"/docs/getting-started/running-nexus/docker.html","text":"","title":"Run Nexus locally with Docker"},{"location":"/docs/getting-started/running-nexus/docker.html#run-nexus-locally-with-docker","text":"","title":"Run Nexus locally with Docker"},{"location":"/docs/getting-started/running-nexus/docker.html#requirements","text":"","title":"Requirements"},{"location":"/docs/getting-started/running-nexus/docker.html#docker","text":"Regardless of your OS, make sure to run a recent version of Docker (community edition). This was tested with versions 18.03.1 and above. You might need to get installation packages directly from the official Docker website if the one provided by your system package manager is outdated.\nCommand docker --version\n Example $ docker version\nDocker version 18.03.1-ce, build 9ee9f40","title":"Docker"},{"location":"/docs/getting-started/running-nexus/docker.html#memory-and-cpu-limits","text":"On macOS and Windows, Docker effectively runs containers inside a VM created by the system hypervisor. Nexus requires at least 2 CPUs and 8 GiB of memory in total. You can increase the limits in Docker settings in the menu Preferences > Advanced.","title":"Memory and CPU limits"},{"location":"/docs/getting-started/running-nexus/docker.html#initialize-docker-swarm","text":"If you’ve never used Docker Swarm or Docker Stacks before, you first need to create a swarm cluster on your local machine:\nCommand docker swarm init\n Example $ docker swarm init\nSwarm initialized: current node (***) is now a manager.\n \nTo add a worker to this swarm, run the following command:\n \ndocker swarm join --token {token} 128.178.97.243:2377\n \nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.","title":"Initialize Docker Swarm"},{"location":"/docs/getting-started/running-nexus/docker.html#deployment","text":"Download the Docker Compose template and the Nginx router configuration into a directory of your choice. For instance ~/docker/nexus/.","title":"Deployment"},{"location":"/docs/getting-started/running-nexus/docker.html#starting-nexus","text":"Create a nexus deployment with Docker Stacks:\nCommand docker stack deploy nexus --compose-file=docker-compose.yaml\n Example $ cd ~/docker/nexus\n$ docker stack deploy nexus --compose-file=docker-compose.yaml\nCreating network nexus_default\nCreating config nexus_nginx\nCreating service nexus_iam\nCreating service nexus_admin\nCreating service nexus_elasticsearch\nCreating service nexus_cassandra\nCreating service nexus_kafka\nCreating service nexus_blazegraph\nCreating service nexus_router\nCreating service nexus_kg\nWait about one minute and you should be able to access Nexus locally, on the port 80:\nCommand curl http://localhost\n Example $ curl http://localhost\n{\"name\":\"kg\",\"version\":\"0.10.11\"}\nTo list running services or access logs, please refer to the documentation.","title":"Starting Nexus"},{"location":"/docs/getting-started/running-nexus/docker.html#stopping-nexus","text":"You can stop and delete the entire deployment with:\nCommand docker stack rm nexus\n Example $ docker stack rm nexus\nRemoving service nexus_admin\nRemoving service nexus_blazegraph\nRemoving service nexus_cassandra\nRemoving service nexus_elasticsearch\nRemoving service nexus_iam\nRemoving service nexus_kafka\nRemoving service nexus_kg\nRemoving service nexus_router\nRemoving config nexus_nginx\nRemoving network nexus_default\nNote As no data is persisted outside the containers, everyting will be lost once you remove the Nexus deployment. If you’d like help with creating persistent volumes, feel free to contact us on our Gitter channel.","title":"Stopping Nexus"},{"location":"/docs/getting-started/running-nexus/minikube.html","text":"","title":"Run Nexus locally with Minikube"},{"location":"/docs/getting-started/running-nexus/minikube.html#run-nexus-locally-with-minikube","text":"Minikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day.\nNote This section makes use of static assets hosted on this website; to remove the clutter please export the base of the documentation to $MINI env var: export MINI=\"https://bluebrain.github.io/nexus/docs/getting-started/running-nexus/minikube\"\nNote This page presents the necessary commands to deploy Nexus with Minikube but also examples the show the expected output. Some of the examples on this page make use of curl (https://curl.haxx.se/) and jq (https://stedolan.github.io/jq/) for formatting the json output when interacting with the services. Please install these command line tools if you’d like to run the commands in the examples. On macOS you can run: brew install curl jq","title":"Run Nexus locally with Minikube"},{"location":"/docs/getting-started/running-nexus/minikube.html#install-minikube","text":"Follow the installation instructions posted on the Minikube project page.","title":"Install Minikube"},{"location":"/docs/getting-started/running-nexus/minikube.html#minikube-install-instructions-for-macos","text":"An example for installing and running Minikube on macOS using Hyperkit after installing Docker for Mac:\nbrew install kubectl\nbrew cask install minikube\ncurl -Lo docker-machine-driver-hyperkit https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-hyperkit \\\n    && chmod +x docker-machine-driver-hyperkit \\\n    && sudo cp docker-machine-driver-hyperkit /usr/local/bin/ \\\n    && rm docker-machine-driver-hyperkit \\\n    && sudo chown root:wheel /usr/local/bin/docker-machine-driver-hyperkit \\\n    && sudo chmod u+s /usr/local/bin/docker-machine-driver-hyperkit\nTo start Minikube run (notice the cpu and memory flags, the setup requires a minimum of --cpus=2 --memory=8196):\nminikube start --cpus 6 --memory 10240 --vm-driver=hyperkit\nIf the installation is successful you can run the following command to open the Kubernetes Dashboard:\nminikube dashboard\nTo stop Minikube run:\nminikube stop\nNote After stopping minikube the vm still exists on the system; starting minikube again will preserve the deployed services. To permanently remove minikube vm run: minikube delete","title":"Minikube install instructions for macOS"},{"location":"/docs/getting-started/running-nexus/minikube.html#enable-the-ingress-addon","text":"Minikube comes with a collection of addons like the Kubernetes Dashboard but not all are enabled by default. An important one is the ingress addon which enables routing http traffic from the host into the cluster.\nTo enable the ingress addon run:\nCommand minikube addons enable ingress\n Example $ minikube addons enable ingress\ningress was successfully enabled\n$\nTo get the external IP of the cluster (to be used later in accessing services) run:\nCommand minikube addons open ingress --url | awk -F / '{print $3}' | awk -F : '{print $1}'\n Example $ minikube addons open ingress --url | awk -F / '{print $3}' | awk -F : '{print $1}'\n192.168.64.3\n$","title":"Enable the ingress addon"},{"location":"/docs/getting-started/running-nexus/minikube.html#setup-a-separate-namespace","text":"Kubernetes namespaces are logical groupings of resources which allow segregating various deployments in “virtual clusters”.\nThe default installation of Minikube creates three namespaces: kube-system, kube-public and default. This example uses a separate namespace to group Nexus specific resources.\nGet the list of available namespaces:\nCommand kubectl get namespaces\n Example $ kubectl get namespaces\nNAME          STATUS    AGE\ndefault       Active    1h\nkube-public   Active    1h\nkube-system   Active    1h\n$\nCreate the nexus namespace:\nCommand kubectl apply -f $MINI/namespace.yaml\n Example $ kubectl apply -f $MINI/namespace.yaml\nnamespace/nexus created\n$ kubectl get namespaces\nNAME          STATUS    AGE\ndefault       Active    1h\nkube-public   Active    1h\nkube-system   Active    1h\nnexus         Active    1m\n$\nDefault the kubectl to the nexus namespace:\nCommand kubectl config set-context minikube --namespace=nexus\n Example $ kubectl config set-context minikube --namespace=nexus\nContext \"minikube\" modified.\n$\nNote Every time Minikube is stopped and started again, the context and its configuration is lost. Remember to run the following commands every time you start minikube: kubectl config use-context minikube && kubectl config set-context minikube --namespace=nexus","title":"Setup a separate namespace"},{"location":"/docs/getting-started/running-nexus/minikube.html#deploy-dependent-services","text":"Nexus uses numerous off the shelf services that need to be set-up as a prerequisite. Run the following command to save the IP address of the minikube cluster in an environment variable:\nCommand export NEXUS=$(minikube addons open ingress --url | awk -F / '{print $3}' | awk -F : '{print $1}')\n Example $ export NEXUS=$(minikube addons open ingress --url | awk -F / '{print $3}' | awk -F : '{print $1}')\n$ echo $NEXUS\n192.168.64.3\n$","title":"Deploy dependent services"},{"location":"/docs/getting-started/running-nexus/minikube.html#deploy-cassandra","text":"Command kubectl apply -f $MINI/cassandra.yaml && \\\n  kubectl wait pod cassandra-0 --for condition=ready --timeout=180s\n Example $ kubectl apply -f $MINI/cassandra.yaml\nservice/cassandra created\nstatefulset.apps/cassandra created\n$ kubectl exec -it cassandra-0 -- nodetool status\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address     Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  172.17.0.4  103.71 KiB  256          100.0%            80c0bdfa-1f5e-41aa-8a7e-f0dea7fe7ef0  rack1\n$","title":"Deploy Cassandra"},{"location":"/docs/getting-started/running-nexus/minikube.html#deploy-elasticsearch","text":"Command kubectl apply -f $MINI/elasticsearch.yaml && \\\n  kubectl wait pod elasticsearch-0 --for condition=ready --timeout=180s\n Example $ kubectl apply -f $MINI/elasticsearch.yaml\nservice/elasticsearch created\nservice/elasticsearch-discovery created\nstatefulset.apps/elasticsearch created\n$ kubectl wait pod elasticsearch-0 --for condition=ready --timeout=60s\npod/elasticsearch-0 condition met\n$ curl \"http://$NEXUS/elasticsearch\"\n{\n  \"name\" : \"elasticsearch-0\",\n  \"cluster_name\" : \"nexus-cluster\",\n  \"cluster_uuid\" : \"pvu_3bdoR0az4_9qIw0wlg\",\n  \"version\" : {\n\"number\" : \"6.3.1\",\n\"build_flavor\" : \"default\",\n\"build_type\" : \"tar\",\n\"build_hash\" : \"eb782d0\",\n\"build_date\" : \"2018-06-29T21:59:26.107521Z\",\n\"build_snapshot\" : false,\n\"lucene_version\" : \"7.3.1\",\n\"minimum_wire_compatibility_version\" : \"5.6.0\",\n\"minimum_index_compatibility_version\" : \"5.0.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n$","title":"Deploy ElasticSearch"},{"location":"/docs/getting-started/running-nexus/minikube.html#deploy-blazegraph","text":"Command kubectl apply -f $MINI/blazegraph.yaml && \\\n  kubectl wait pod blazegraph-0 --for condition=ready --timeout=180s\n Example $ kubectl apply -f $MINI/blazegraph.yaml\nservice/blazegraph created\nstatefulset.apps/blazegraph created\npersistentvolumeclaim/storage-blazegraph created\ningress.extensions/blazegraph created\n$ kubectl wait pod blazegraph-0 --for condition=ready --timeout=180s\npod/blazegraph-0 condition met\n$ curl -s -H\"Accept: application/json\" \"http://$NEXUS/blazegraph/namespace?describe-each-named-graph=false\" | head -4\n  {\n\"head\" : {\n  \"vars\" : [ \"subject\", \"predicate\", \"object\", \"context\" ]\n},\n$","title":"Deploy BlazeGraph"},{"location":"/docs/getting-started/running-nexus/minikube.html#deploy-kafka","text":"Command kubectl apply -f $MINI/zookeeper.yaml && \\\n  kubectl wait pod zookeeper-0 --for condition=ready --timeout=180s && \\\n  kubectl apply -f $MINI/kafka.yaml && \\\n  kubectl wait pod kafka-0 --for condition=ready --timeout=180s\n Example $ kubectl apply -f $MINI/zookeeper.yaml && \\\n  kubectl wait pod zookeeper-0 --for condition=ready --timeout=180s && \\\n  kubectl apply -f $MINI/kafka.yaml && \\\n  kubectl wait pod kafka-0 --for condition=ready --timeout=180s\nservice/zookeeper created\nstatefulset.apps/zookeeper created\npod/zookeeper-0 condition met\nservice/kafka created\nstatefulset.apps/kafka created\npod/kafka-0 condition met\n$","title":"Deploy Kafka"},{"location":"/docs/getting-started/running-nexus/minikube.html#deploy-nexus-services","text":"Before configuring the services a configuration map must first be created that keeps track of the “public” ip address of the minikube cluster. The following command will replace the {NEXUS} token in the config.yaml file with the value stored in the $NEXUS variable set above.\nCommand curl -s $MINI/config.yaml | sed \"s/{NEXUS}/$NEXUS/g\" | kubectl apply -f -\n Example $ curl -s $MINI/config.yaml | sed \"s/{NEXUS}/$NEXUS/g\" | kubectl apply -f -\nconfigmap/config created\n$ kubectl get configmap/config -o yaml | grep public.ip:\n  public.ip: 192.168.64.4\n$","title":"Deploy Nexus Services"},{"location":"/docs/getting-started/running-nexus/minikube.html#deploy-iam","text":"IAM is the service that manages identities and tokens via downstream OIDC providers and manages the permissions to arbitrary resources in the system.\nCommand kubectl apply -f $MINI/iam.yaml && \\\n  kubectl wait pod iam-0 --for condition=ready --timeout=180s\n Example $ kubectl apply -f $MINI/iam.yaml\nservice/iam created\nservice/iam-hd created\nstatefulset.apps/iam created\ningress.extensions/iam created\ningress.extensions/iam-direct created\n$ kubectl wait pod iam-0 --for condition=ready --timeout=180s\npod/iam-0 condition met\n$ curl -s \"http://$NEXUS/iam\" | jq\n{\n  \"name\": \"iam\",\n  \"version\": \"0.10.21\",\n  \"_links\": [\n    {\n      \"rel\": \"api\",\n      \"href\": \"http://192.168.64.6/v1/acls\"\n    }\n  ]\n}\n$ curl -s \"http://$NEXUS/v1/acls/\" | jq\n{\n  \"@context\": \"http://192.168.64.6/v1/contexts/nexus/core/iam/v0.1.0\",\n  \"acl\": [\n    {\n      \"path\": \"/\",\n      \"identity\": {\n        \"@id\": \"http://192.168.64.6/v1/anonymous\",\n        \"@type\": \"Anonymous\"\n      },\n      \"permissions\": [\n        \"read\",\n        \"own\"\n      ]\n    }\n  ]\n}\n$","title":"Deploy IAM"},{"location":"/docs/getting-started/running-nexus/minikube.html#deploy-admin","text":"Admin is the service that manages accounts (orgs and users) and projects their configuration.\nCommand kubectl apply -f $MINI/admin.yaml && \\\n  kubectl wait pod admin-0 --for condition=ready --timeout=180s\n Example $ kubectl apply -f $MINI/admin.yaml\nservice/admin created\nservice/admin-hd created\nstatefulset.apps/admin created\ningress.extensions/admin created\ningress.extensions/admin-direct created\n$ kubectl wait pod admin-0 --for condition=ready --timeout=180s\npod/admin-0 condition met\n$ curl -s \"http://$NEXUS/admin\" | jq\n{\n  \"name\": \"admin\",\n  \"version\": \"0.2.7\"\n}\n$ curl -s \"http://$NEXUS/v1/projects\" | jq # the access error is expected\n{\n  \"@context\": \"http://bluebrain.github.io/nexus/contexts/error.json\",\n  \"code\": \"UnauthorizedAccess\"\n}\n$","title":"Deploy Admin"},{"location":"/docs/getting-started/running-nexus/minikube.html#deploy-kg","text":"KG is the service that manages user defined resources, their schemas and configuration like resolvers, views etc.\nCommand kubectl apply -f $MINI/kg.yaml && \\\n  kubectl wait pod kg-0 --for condition=ready --timeout=180s\n Example $ kubectl apply -f $MINI/kg.yaml\nservice/kg created\nservice/kg-hd created\nstatefulset.apps/kg created\npersistentvolumeclaim/storage-kg created\ningress.extensions/kg created\ningress.extensions/kg-direct created\n$ kubectl wait pod kg-0 --for condition=ready --timeout=180s\npod/kg-0 condition met\n$ curl -s \"http://$NEXUS/kg\" | jq\n{\n  \"name\": \"kg\",\n  \"version\": \"0.10.11\"\n}\n$ curl -s \"http://$NEXUS/v1/resources/org/proj\" | jq # the access error is expected\n{\n  \"@context\": \"https://bluebrain.github.io/nexus/contexts/error\",\n  \"code\": \"UnauthorizedAccess\"\n}\n$","title":"Deploy KG"},{"location":"/docs/getting-started/running-nexus/on-premise-cloud.html","text":"","title":"On premise / cloud deployment"},{"location":"/docs/getting-started/running-nexus/on-premise-cloud.html#on-premise-cloud-deployment","text":"There are several things to consider when preparing to deploy Nexus “on premise” because the setup depends a lot on the various usage profiles, but the most important categories would be:\nAvailability Latency & throughput Capacity Efficient use of hardware resources Backup and restore Monitoring & alerting\nEach of the Nexus services and “off the shelf” products can be deployed as a single instance or as a cluster (with one exception at this point being BlazeGraph which doesn’t come with a clustering option). The advantages for deploying clusters are generally higher availability, capacity and throughput at the cost of higher latency, consistency and having to potentially deal with network instability.\nThe decision to go with single node deployments or clustered deployments can be revisited later on and mixed setups (some services single node while others clustered) are also possible.\nThe Nexus distribution is made up of docker images which can be run on any host operating system and each of the “off the shelf” products also offer docker as a deployment option. We would generally recommend using a container orchestration solution like Kubernetes, OpenShift or Docker Swarm as they offer good management capabilities, discovery, load balancing and self-healing. They also accommodate changes in hardware allocations for the deployments, changes that can occur due to evolving usage patterns, software updates etc. Currently the biggest Nexus deployment is at EPFL within OpenShift.","title":"On premise / cloud deployment"},{"location":"/docs/getting-started/running-nexus/on-premise-cloud.html#choice-of-hardware","text":"Depending on the target throughput, usage profiles and data volume the hardware specification can vary greatly; please take a look at the benchmarks section to get an idea of what you should expect in terms of throughput with various hardware configurations. When the usage profiles are unknown a couple of rules of thumb should narrow the scope:\nNexus uses a collection of data stores (Cassandra, ElasticSearch, BlazeGraph) which depend performance wise to the underlying disk access, so: prefer local storage over network storage for lower latency when doing IO, prefer SSD over HDDs because random access speed is more important than sequential access, one exception is the file storage (attachments to resources which are stored as binary blobs on the filesystem) where the network disks should not be a cause for concern, nor random access speed; this assumes that accessing attachments is not the at the top in the usage profile All of Nexus services and most of the “off the shelf” products are built to run on top of the JVM which usually require more memory over computing power. A rough ratio of 2 CPU cores per 8GB of RAM is probably a good one (this of course depends on the CPU specification). Due to the design for scalability of Nexus services and “off the shelf” products the network is a very important characteristic of the deployment as frequent dropped packets or network partitions can seriously affect the availability of the system. Clustered / distributed systems generally use some form of consensus which is significantly affected by the reliability of the network. If the reliability of the network is a concern within the target deployment then vertical scalability is desirable over horizontal scalability: fewer host nodes with better specifications is better over more commodity hardware host nodes.","title":"Choice of hardware"},{"location":"/docs/getting-started/running-nexus/on-premise-cloud.html#cassandra","text":"Nexus uses Cassandra as its primary store as it scales well in terms of reads with the number of nodes in the cluster. It offers data replication out of the box, which allows the system to continue to be available in case of node failures or network partitions.\nSince this is the primary store it is the most important system to be backed up. All of the data that Nexus uses in other stores can be recomputed from the one stored in Cassandra as the other stores are used as mere indexing systems.\nPlease have a look at the Planning and Testing section in the DataStax documentation as it contains recommendations in terms of hardware and capacity.\nAs described in the architecture section the generally adopted persistence model is an EventSourced model in which the data store is used as an append only store. This has implications to the total amount of disk used by the primary store.\nA formula for computing the required disk space:\ntotal = (resource_size + nexus_metadata_size) * count * number_updates * replication_factor * 2 (compaction requirement)\nThe nexus_metadata_size varies depending on many things, but it’s generally less than or equal to the resource_size.\nAn example, assuming:\n10KB per resource 1.000.000 distinct resources 10 updates per resource replication factor of 3\n… the total required disk size would be:\n(10KB + 10KB) * 1.000.000 * 10 * 3 * 2 = 1.000.000.000KB ~= 955GB\nThe resulting size represents the total disk space of the cluster; a 5 node cluster with the data volume in the example above would have to be configured with 200GB disks per node.","title":"Cassandra"},{"location":"/docs/getting-started/running-nexus/on-premise-cloud.html#elasticsearch","text":"Nexus uses ElasticSearch to host several system indices and user defined ones. It offers sharding and replication out of the box. Deciding whether this system requires backup depends on the tolerated time for a restore. Nexus can be instructed to rebuild all indices using the data from the primary store, but being an incremental indexing process it can take longer than restoring from a backup. Since it can be configured to host a number of replicas for each shard it can tolerate a number of node failures.\nThe ElasticSearch setup documentation contains the necessary information on how to install and configure it, but recommendations on sizing the nodes and cluster are scarce because it depends on usage.\nA formula for computing the required disk space:\ntotal = (resource_size * count * documents + lucene_index) * replication_factor\n… where the lucene_index while it can vary should be less than twice the size of the original documents.\nAn example, assuming:\n10KB per resource 1.000.000 distinct resources 3 documents per resource (the number of documents depends on the configured views in the system) 2 additional shard replicas (replication factor of 3)\n… the total required disk size would be:\n(10KB * 1.000.000 * 3 + 2 * (10KB * 1.000.000 * 3)) * 3 = 270.000.000KB ~= 260GB\nThe resulting size represents the total disk space of the data nodes in the cluster; a 5 data node cluster with the data volume in the example above would have to be configured with 60GB disks per node.","title":"ElasticSearch"},{"location":"/docs/getting-started/running-nexus/on-premise-cloud.html#kafka-and-zookeeper","text":"Nexus uses Kafka for asynchronous communication between services, usually exposing the event log directly with simple transformations (internal service event representation to public representation). Kafka is also used to provide an integration point such that custom / specialized services can be built to run on top of Nexus. It offers replication out of the box and while the event log can be rebuilt from the primary store whether it requires backup or not is a decision that depends on how quickly a restore needs to be performed in case of failure.\nThe system is not used at its full potential in terms of throughput, a small cluster will work on most Nexus deployments. It was chosen because of its similarity with the Nexus service event based persistence model.\nNexus doesn’t use ZooKeeper directly, but just as a dependency for Kafka which in turn is used solely for coordinating the Kafka cluster. A 3 node 0.5 CPU / 1.5GB RAM cluster should be sufficient for most use cases.\nThe Kafka configuration section in its documentation list a series of recommendations and instructions for a production deployment.\nThe ZooKeeper’s Getting Started and Administrator guides are a good place to start.\nNexus pushes to Kafka each service event log, so the allocated disk space is an important aspect to take into consideration. As with previous sizing instructions the disk size depends on the number of resources, their size and the configured replication factor:\ntotal = (resource_size + nexus_metadata_size) * count * replication_factor\nAn example, assuming:\n10KB per resource 1.000.000 distinct resources 10 updates per resource replication factor of 3\n… the total required disk size would be:\n(10KB + 10KB) * 1.000.000 * 3 = 60.000.000KB ~= 60GB\nThe resulting size represents the total disk space of the cluster; a 3 data node cluster with the data volume in the example above would have to be configured with 20GB disks per node.\nNote Nexus stores its entire event log in Kafka so it’s important to configure Kafka with permanent log retention. Make sure the following configuration values are set: log.retention.bytes=-1\nlog.retention.hours=-1","title":"Kafka and ZooKeeper"},{"location":"/docs/getting-started/running-nexus/on-premise-cloud.html#blazegraph","text":"Nexus uses BlazeGraph as an RDF (triple) store to provide a advanced querying capabilities on the hosted data. This store is treated as a specialized index on the data so as with Kafka and ElasticSearch in case of failures, the system can be fully restored from the primary store. While the technology is advertised to support High Availability and Scaleout deployment configurations, we have yet to be able to setup a deployment in this fashion.\nWe currently recommend deploying BlazeGraph using the prepackaged tar.gz distribution available to download from sourceforge.\nNote We’re looking at alternative technologies and possible application level (within Nexus) sharding and replicas.\nThe Hardware Configuration section in the documentation gives a couple of hints about the requirements to operate BlazeGraph and there are additional sections for optimizations in terms of Performance, IO and Query.\nBlazeGraph stores data in an append only journal which means updates will use additional disk space.\nA formula for computing the required disk space:\ntotal = (resource_triples + nexus_triples) * count * number_updates * triple_size + lucene_index\n… where the lucene_index while it can vary should be less than twice the size of the original documents.\nAn example, assuming:\n100 triples (rough estimate for a 10KB json-ld resource representation) 20 additional nexus triples on average 1.000.000 distinct resources 10 updates per resource 200 bytes triple size (using quads mode)\n… the total required disk size would be:\n(100 + 20) * 1.000.000 * 10 * 200 / 1024 * 3 ~= 700.000.000KB ~= 670GB\nCompactions can be applied to the journal using the CompactJournalUtility to reduce the disk usage, but it takes quite a bit a time and requires taking the software offline during the process.","title":"BlazeGraph"},{"location":"/docs/getting-started/intro-linked-data.html","text":"","title":"Introduction to Linked Data"},{"location":"/docs/getting-started/intro-linked-data.html#introduction-to-linked-data","text":"TBC.","title":"Introduction to Linked Data"},{"location":"/docs/getting-started/shacl.html","text":"","title":"Data validation with SHACL"},{"location":"/docs/getting-started/shacl.html#data-validation-with-shacl","text":"TBC.","title":"Data validation with SHACL"},{"location":"/docs/getting-started/example.html","text":"","title":"Data management example"},{"location":"/docs/getting-started/example.html#data-management-example","text":"TBC.","title":"Data management example"},{"location":"/docs/api/index.html","text":"","title":"API Reference"},{"location":"/docs/api/index.html#api-reference","text":"TBC.","title":"API Reference"},{"location":"/docs/api/operating-on-resources.html","text":"","title":"Operating on resources"},{"location":"/docs/api/operating-on-resources.html#operating-on-resources","text":"Any resources in the system might be protected using an access token, provided by the HTTP header Authorization: Bearer {access_token}. Visit Authentication in order to learn more about how to retrieve an access token.\nAll resources in the system share a base set of operations. Assuming a nexus deployment at http(s)://nexus.example.com resource address of /v1/{address} the following operations should apply to most (all) resources:","title":"Operating on resources"},{"location":"/docs/api/operating-on-resources.html#fetch-the-current-revision-of-the-resource","text":"GET /v1/{address}","title":"Fetch the current revision of the resource"},{"location":"/docs/api/operating-on-resources.html#status-codes","text":"200 OK: the resource is found and returned successfully 404 Not Found: the resource was not found","title":"Status Codes"},{"location":"/docs/api/operating-on-resources.html#fetch-a-specific-revision-of-the-resource","text":"GET /v1/{address}?rev={rev}\n… where {rev} is the revision number, starting at 1.","title":"Fetch a specific revision of the resource"},{"location":"/docs/api/operating-on-resources.html#status-codes","text":"200 OK: the resource revision is found and returned successfully 404 Not Found: the resource revision was not found","title":"Status Codes"},{"location":"/docs/api/operating-on-resources.html#fetch-a-specific-tag-of-the-resource","text":"GET /v1/{address}?tag={tag}\n… where {tag} is the tag name linked to a certain revision number.","title":"Fetch a specific tag of the resource"},{"location":"/docs/api/operating-on-resources.html#status-codes","text":"200 OK: the resource tag is found and returned successfully 404 Not Found: the resource tag was not found","title":"Status Codes"},{"location":"/docs/api/operating-on-resources.html#create-a-new-resource","text":"Depending on whether the resource is a singleton resource or is part of a wider collection of resources of the same type the verbs POST and PUT are used.\nFor a singleton resource:\nPUT /v1/{address}\n{...}\nFor a collection resources:\nPOST /v1/{collection_address}\n{...}\n… where {collection_address} is the address of the collection the resource belongs to.","title":"Create a new resource"},{"location":"/docs/api/operating-on-resources.html#status-codes","text":"201 Created: the resource was created successfully 400 Bad Request: the resource is not valid or cannot be created at this time 409 Conflict: the resource already exists","title":"Status Codes"},{"location":"/docs/api/operating-on-resources.html#update-a-resource","text":"In order to ensure a client does not perform any changes to a resource without having had seen the previous revision of the resource, the last revision needs to be passed as a query parameter.\nPUT /v1/{address}?rev={previous_rev}\n{...}","title":"Update a resource"},{"location":"/docs/api/operating-on-resources.html#status-codes","text":"200 OK: the resource was created successfully 400 Bad Request: the resource is not valid or cannot be updated at this time 409 Conflict: the provided revision is not the current resource revision number","title":"Status Codes"},{"location":"/docs/api/operating-on-resources.html#tag-a-resource","text":"Links a resource revision to a specific name.\nTagging a resource is considered to be an update as well.\nPUT /v1/{address}/tags?rev={previous_rev}\n{\n   \"tag\": \"{name}\",\n   \"rev\": {rev}\n}\n… where:\n{name} the name to give to the resource at a specific revision. {rev} the revision number to link the provided {name}.","title":"Tag a resource"},{"location":"/docs/api/operating-on-resources.html#deprecate-a-resource","text":"Locks the resource, so no further operations can be performed. It also deletes the resource from listing/querying results.\nDeprecating a resource is considered to be an update as well.\nDELETE /v1/{address}?rev={previous_rev}","title":"Deprecate a resource"},{"location":"/docs/api/operating-on-resources.html#status-codes","text":"200 OK: the resource was created successfully 400 Bad Request: the resource is not valid or cannot be deprecated at this time 409 Conflict: the provided revision is not the current resource revision number","title":"Status Codes"},{"location":"/docs/api/operating-on-resources.html#status-codes","text":"200 OK: the resource was created successfully 400 Bad Request: the resource is not valid or cannot be created at this time 409 Conflict: the provided revision is not the current resource revision number","title":"Status Codes"},{"location":"/docs/api/operating-on-resources.html#listing","text":"GET /v1/{collection_address}?from={from}&size={size}&deprecated={deprecated}&q={full_text_search_query}\n… where all of the query parameters are individually optional.\n{collection_address} is the selected collection to list, filter or search; for example: /v1/projects/, /v1/schemas/{account}/{project}, {full_text_search_query}: String - can be provided to select only the resources in the collection that have attribute values matching (containing) the provided token; when this field is provided the results will also include score values for each result {from}: Number - is the parameter that describes the offset for the current query; defaults to 0 {size}: Number - is the parameter that limits the number of results; defaults to 20 {deprecated}: Boolean - can be used to filter the resulting resources based on their deprecation status","title":"Listing"},{"location":"/docs/api/operating-on-resources.html#list-response-format","text":"The response to any search requests follows the described format:\n\"_total\": {hits},\n  \"_maxScore\": {maxScore},\n  \"_next\": \"{next_page_address}\",\n  \"_previous\": \"{previous_page_address}\",\n  \"_results\": [\n    {\n      \"@id\": \"{resource_id}\",\n      ...\n    },\n    {\n      \"@id\": \"{resource_id}\",\n      ...\n    }\n  ]\n… where:\n{hits} is the total number of results found for the requested search. {maxScore} is the maximum score found across all hits. {resource_id} is the qualified id for one of the results.\nThe relationships _next and _previous at the top level offer discovery of more resources, in terms of navigation/pagination.\nThe fields {maxScore} and {score_id} are optional fields and will only be present whenever a q query parameter is provided on the request.","title":"List response format"},{"location":"/docs/api/admin-service-api.html","text":"","title":"Admin Service API"},{"location":"/docs/api/admin-service-api.html#admin-service-api","text":"TBC.","title":"Admin Service API"},{"location":"/docs/api/kg/index.html","text":"","title":"KnowledgeGraph Service API"},{"location":"/docs/api/kg/index.html#knowledgegraph-service-api","text":"","title":"KnowledgeGraph Service API"},{"location":"/docs/api/kg/index.html#resources","text":"A resource is an instance on the Knowledge Graph.\nOperations on resources","title":"Resources"},{"location":"/docs/api/kg/index.html#schemas","text":"A schema is a resource which defines a set of rules and constrains using SHACL.\nOperations on schemas","title":"Schemas"},{"location":"/docs/api/kg/index.html#resolvers","text":"A resolver s a resource which defines the way ids are resolved inside a project.\nOperations on resolvers","title":"Resolvers"},{"location":"/docs/api/kg/index.html#views","text":"A view is a resource which defines the way indexing is applied to the resources inside a project.\nOperations on views","title":"Views"},{"location":"/docs/api/kg/kg-resources-api.html","text":"","title":"Resources"},{"location":"/docs/api/kg/kg-resources-api.html#resources","text":"Generic resources are rooted in the /v1/resources/{accountId}/{projId}/{schema_id} collection.\nEach resource…\nbelongs to a project ({projId}) inside an account ({accountId}) it is validated against a schema ({schema_id}).","title":"Resources"},{"location":"/docs/api/kg/kg-resources-api.html#create-a-resource-using-post","text":"POST /v1/resources/{accountId}/{projId}/{schema_id}\n  {...}\n ```\n\nThe json payload: \n\n- If the `@id` value is found on the payload, this @id will be used.\n- If the `@id` value is not found on the payload, an @id will be generated as follows: `base:{UUID}`. The `base` is the `prefix` defined on the resource's project (`{projId}`).\n\n### Example\n\nRequest\n:   @@snip [resource.sh](../assets/resource.sh)\n\nPayload\n:   @@snip [resource.json](../assets/resource.json)\n\nResponse\n:   @@snip [resource-ref-new.json](../assets/resource-ref-new.json)\n\n\n## Create a resource using PUT\nThis alternative endpoint to create a resource is useful in case the json payload does not contain an `@id` but you want to specify one. The @id will be specified in the last segment of the endpoint URI.\nPUT /v1/resources/{accountId}/{projId}/{schema_id}/{resource_id} {…}\nThe json payload. Note that if the payload contains an @id different from the `{resource_id}`, the request will fail.\n\n### Example\n\nRequest\n:   @@snip [resource-put.sh](../assets/resource-put.sh)\n\nPayload\n:   @@snip [resource.json](../assets/resource.json)\n\nResponse\n:   @@snip [resource-ref-new.json](../assets/resource-ref-new.json)\n\n\n## Update a resource\n\nThis operation overrides the payload.\n\nIn order to ensure a client does not perform any changes to a resource without having had seen the previous revision of\nthe resource, the last revision needs to be passed as a query parameter.\nPUT /v1/resources/{accountId}/{projId}/{schema_id}/{resource_id}?rev={previous_rev} {…}\n... where `{previous_rev}` is the last known revision number for the resource.\n\n\n### Example\n\nRequest\n:   @@snip [resource-update.sh](../assets/resource-update.sh)\n\nPayload\n:   @@snip [resource.json](../assets/resource.json)\n\nResponse\n:   @@snip [resource-ref-new-updated.json](../assets/resource-ref-new-updated.json)\n\n\n## Tag a resource\n\nLinks a resource revision to a specific name. \n\nTagging a resource is considered to be an update as well.\nPUT /v1/resources/{accountId}/{projId}/{schema_id}/{resource_id}/tags?rev={previous_rev} { “tag”: “{name}”, “rev”: {rev} }\n... where \n\n- `{previous_rev}` is the last known revision number for the resource.\n- `{name}` the name to give to the resource at a specific revision.\n- `{rev}` the revision number to link the provided `{name}`.\n\n### Example\n\nRequest\n:   @@snip [resource-tag.sh](../assets/resource-tag.sh)\n\nPayload\n:   @@snip [tag.json](../assets/tag.json)\n\nResponse\n:   @@snip [resource-ref-new-tagged.json](../assets/resource-ref-new-tagged.json)\n\n\n## Deprecate a resource\n\nLocks the resource, so no further operations can be performed. It also deletes the resource from listing/querying results.\n\nDeprecating a resource is considered to be an update as well.\nDELETE /v1/resources/{accountId}/{projId}/{schema_id}/{resource_id}?rev={previous_rev}\n... where `{previous_rev}` is the last known revision number for the resource.\n\n### Example\n\nRequest\n:   @@snip [resource-deprecate.sh](../assets/resource-deprecate.sh)\n\nResponse\n:   @@snip [resource-ref-new-deprecated.json](../assets/resource-ref-new-deprecated.json)\n\n\n## Fetch a resource (current version)\nGET /v1/resources/{accountId}/{projId}/{schema_id}/{resource_id}\n### Example\n\nRequest\n:   @@snip [resource-fetch.sh](../assets/resource-fetch.sh)\n\nResponse\n:   @@snip [resource-fetched.json](../assets/resource-fetched.json)\n\n\n## Fetch a resource (specific version)\nGET /v1/resources/{accountId}/{projId}/{schema_id}/{resource_id}?rev={rev}\n... where `{rev}` is the revision number of the resource to be retrieved.\n\n### Example\n\nRequest\n:   @@snip [resource-fetch-revision.sh](../assets/resource-fetch-revision.sh)\n\nResponse\n:   @@snip [resource-fetched.json](../assets/resource-fetched.json)\n\n\n## Fetch a resource (specific tag)\nGET /v1/resources/{accountId}/{projId}/{schema_id}/{resource_id}?tag={tag} ```\n… where {tag} is the tag of the resource to be retrieved.","title":"Create a resource using POST"},{"location":"/docs/api/kg/kg-resources-api.html#example","text":"Request curl \"https://nexus.example.com/v1/resources/myaccount/myproj/myschema/base:fd8a2b32-170e-44e8-808f-44a8cbbc49b0?tag=mytag\"Full source at GitHub Response {\n  \"@context\": [\n    {\n      \"@vocab\": \"http://example.com/\",\n      \"ex\": \"http://example.com/\"\n    },\n    \"https://bluebrain.github.io/nexus/contexts/resource\"\n  ],\n  \"@id\": \"https://nexus.example.com/v1/resources/myaccount/myproj/fd8a2b32-170e-44e8-808f-44a8cbbc49b0\",\n  \"@type\": \"Custom\",\n  \"_self\": \"https://nexus.example.com/v1/resources/myaccount/myproj/myschema/base:fd8a2b32-170e-44e8-808f-44a8cbbc49b0\",\n  \"_constrainedBy\": \"nxs:resource\",\n  \"_project\": \"https://nexus.example.com/v1/projects/myaccount/myproj\",\n  \"_createdAt\": \"2018-09-17T14:54:42.939Z\",\n  \"_createdBy\": \"https://nexus.example.com/v1/realms/myrealm/users/f:ad46ddd6-134e-44d6-ab70-bdf00f48dfce:someone\",\n  \"_updatedAt\": \"2018-09-17T14:54:42.939Z\",\n  \"_updatedBy\": \"https://nexus.example.com/v1/realms/myrealm/users/f:ad46ddd6-134e-44d6-ab70-bdf00f48dfce:someone\",\n  \"_rev\": 1,\n  \"_deprecated\": false\n}Full source at GitHub","title":"Example"},{"location":"/docs/api/kg/kg-schemas-api.html","text":"","title":"Schemas"},{"location":"/docs/api/kg/kg-schemas-api.html#schemas","text":"Schemas are rooted in the /v1/schemas/{accountId}/{projId} collection.\nEach schema…\nbelongs to a project ({projId}) inside an account ({accountId}) it is validated against the shacl schema (version 20170720).","title":"Schemas"},{"location":"/docs/api/kg/kg-schemas-api.html#create-a-schema-using-post","text":"POST /v1/schemas/{accountId}/{projId}\n  {...}\n ```\n\nThe json payload: \n\n- If the `@id` value is found on the payload, this @id will be used.\n- If the `@id` value is not found on the payload, an @id will be generated as follows: `base:{UUID}`. The `base` is the `prefix` defined on the resource's project (`{projId}`).\n\n### Example\n\nRequest\n:   @@snip [schema.sh](../assets/schema.sh)\n\nPayload\n:   @@snip [schema.json](../assets/schema.json)\n\nResponse\n:   @@snip [schema-ref-new.json](../assets/schema-ref-new.json)\n\n\n## Create a schema using PUT\nThis alternative endpoint to create a schema is useful in case the json payload does not contain an `@id` but you want to specify one. The @id will be specified in the last segment of the endpoint URI.\nPUT /v1/schemas/{accountId}/{projId}/{schema_id} {…}\nThe json payload. Note that if the payload contains an @id different from the `{schema_id}`, the request will fail.\n\n### Example\n\nRequest\n:   @@snip [schema-put.sh](../assets/schema-put.sh)\n\nPayload\n:   @@snip [schema.json](../assets/schema.json)\n\nResponse\n:   @@snip [schema-ref-new.json](../assets/schema-ref-new.json)\n\n\n## Update a schema\n\nThis operation overrides the payload.\n\nIn order to ensure a client does not perform any changes to a resource without having had seen the previous revision of\nthe resource, the last revision needs to be passed as a query parameter.\nPUT /v1/schemas/{accountId}/{projId}/{schema_id}?rev={previous_rev} {…}\n... where `{previous_rev}` is the last known revision number for the schema.\n\n\n### Example\n\nRequest\n:   @@snip [schema-update.sh](../assets/schema-update.sh)\n\nPayload\n:   @@snip [schema.json](../assets/schema.json)\n\nResponse\n:   @@snip [schema-ref-new-updated.json](../assets/schema-ref-new-updated.json)\n\n\n## Tag a schema\n\nLinks a schema revision to a specific name. \n\nTagging a schema is considered to be an update as well.\nPUT /v1/schemas/{accountId}/{projId}/{schema_id}/tags?rev={previous_rev} { “tag”: “{name}”, “rev”: {rev} }\n... where \n\n- `{previous_rev}` is the last known revision number for the schema.\n- `{name}` the name to give to the schema at a specific revision.\n- `{rev}` the revision number to link the provided `{name}`.\n\n### Example\n\nRequest\n:   @@snip [schema-tag.sh](../assets/schema-tag.sh)\n\nPayload\n:   @@snip [tag.json](../../assets/tag.json)\n\nResponse\n:   @@snip [schema-ref-new-tagged.json](../assets/schema-ref-new-tagged.json)\n\n\n## Deprecate a schema\n\nLocks the schema, so no further operations can be performed. It also deletes the schema from listing/querying results.\n\nDeprecating a schema is considered to be an update as well.\nDELETE /v1/schemas/{accountId}/{projId}/{schema_id}?rev={previous_rev}\n... where `{previous_rev}` is the last known revision number for the schema.\n\n### Example\n\nRequest\n:   @@snip [schema-deprecate.sh](../assets/schema-deprecate.sh)\n\nResponse\n:   @@snip [schema-ref-new-deprecated.json](../assets/schema-ref-new-deprecated.json)\n\n\n## Fetch a schema (current version)\nGET /v1/schemas/{accountId}/{projId}/{schema_id}\n### Example\n\nRequest\n:   @@snip [schema-fetch.sh](../assets/schema-fetch.sh)\n\nResponse\n:   @@snip [schema-fetched.json](../assets/schema-fetched.json)\n\n\n## Fetch a schema (specific version)\nGET /v1/schemas/{accountId}/{projId}/{schema_id}?rev={rev}\n... where `{rev}` is the revision number of the schema to be retrieved.\n\n### Example\n\nRequest\n:   @@snip [schema-fetch-revision.sh](../assets/schema-fetch-revision.sh)\n\nResponse\n:   @@snip [schema-fetched.json](../assets/schema-fetched.json)\n\n\n## Fetch a schema (specific tag)\nGET /v1/schemas/{accountId}/{projId}/{schema_id}?tag={tag} ```\n… where {tag} is the tag of the resource to be retrieved.","title":"Create a schema using POST"},{"location":"/docs/api/kg/kg-schemas-api.html#example","text":"Request curl \"https://nexus.example.com/v1/schemas/myaccount/myproj/base:e1729302-35b8-4d80-97b2-d63c984e2b5c?tag=mytag\"Full source at GitHub Response {\n  \"@context\": [\n    \"https://bluebrain.github.io/nexus/contexts/shacl-20170720\",\n    {\n      \"ex\": \"http://example.com/\",\n      \"this\": \"https://nexus.example.com/v1/resources/myaccount/myproj/e1729302-35b8-4d80-97b2-d63c984e2b5c/shapes\"\n    },\n    \"https://bluebrain.github.io/nexus/contexts/resource\"\n  ],\n  \"@id\": \"nxs:myschema2\",\n  \"@type\": \"nxv:Schema\",\n  \"shapes\": [\n    {\n      \"@id\": \"this:MyShape\",\n      \"@type\": \"NodeShape\",\n      \"nodeKind\": \"BlankNode:OrIRI\",\n      \"property\": [\n        {\n          \"datatype\": \"xsd:string\",\n          \"minCount\": 1,\n          \"path\": \"ex:name\"\n        },\n        {\n          \"datatype\": \"xsd:boolean\",\n          \"minCount\": 1,\n          \"path\": \"ex:bool\"\n        },\n        {\n          \"datatype\": \"xsd:integer\",\n          \"minCount\": 1,\n          \"path\": \"ex:number\"\n        }\n      ],\n      \"targetClass\": \"ex:Custom\"\n    }\n  ],\n  \"@id\": \"https://nexus.example.com/v1/resources/myaccount/myproj/e1729302-35b8-4d80-97b2-d63c984e2b5c\",\n  \"@type\": \"http://example.com/Custom\",\n  \"_self\": \"https://nexus.example.com/v1/schemas/myaccount/myproj/base:e1729302-35b8-4d80-97b2-d63c984e2b5c\",\n  \"_constrainedBy\": \"nxs:shacl\",\n  \"_project\": \"https://nexus.example.com/v1/projects/myaccount/myproj\",\n  \"_createdAt\": \"2018-09-17T14:54:42.939Z\",\n  \"_createdBy\": \"https://nexus.example.com/v1/realms/myrealm/users/f:ad46ddd6-134e-44d6-ab70-bdf00f48dfce:someone\",\n  \"_updatedAt\": \"2018-09-17T14:54:42.939Z\",\n  \"_updatedBy\": \"https://nexus.example.com/v1/realms/myrealm/users/f:ad46ddd6-134e-44d6-ab70-bdf00f48dfce:someone\",\n  \"_rev\": 1,\n  \"_deprecated\": false\n}Full source at GitHub","title":"Example"},{"location":"/docs/api/kg/kg-resolvers-api.html","text":"","title":"Resolvers"},{"location":"/docs/api/kg/kg-resolvers-api.html#resolvers","text":"Resolvers are rooted in the /v1/resolvers/{accountId}/{projId} collection.\nResolvers are used in the following scenarios\nBring the content of the value on the owl:imports predicate on schemas. The value is the @id of the resource. E.g.: You can find owl imports on a schema, as follows \"owl:imports\": \"http://example.com/myid\"\". The resolver will try to find a resource with \"@id\": \"http://example.com/myid\" and if found, will bring the payload into the original resource. Bring the content of the @context links. The value is the @id of the resource. E.g.: A resource might define the context as follows: \"@context\": \"http://example.com/id\". The resolver will try to find a resource with \"@id\": \"http://example.com/id\" and if found, will bring the payload into the original resource.\nEach resolver…\nbelongs to a project ({projId}) inside an account ({accountId}) it is validated against the resolver schema.","title":"Resolvers"},{"location":"/docs/api/kg/kg-resolvers-api.html#resolver-types","text":"There are several types of resolvers, which perform resolution in different scopes.","title":"Resolver types"},{"location":"/docs/api/kg/kg-resolvers-api.html#inproject-resolver","text":"The scope of the resolution is the current project where the resource resides. In other words:\nSchema A can import schema B using the owl:import as long as schema B is located on the same project as schema A. Resource A can reference resource’s context B (inside @context) as long as resource B is located on the same project as resource A.\nThis resolver gets automatically created when the project is created and it cannot be modified.","title":"InProject resolver"},{"location":"/docs/api/kg/kg-resolvers-api.html#inproject-resolver-payload","text":"{\n    \"@id\": \"nxv:InProject\",\n    \"@type\": [ \"InProject\", \"Resolver\" ],\n    \"priority\": {priority},\n}\nwhere {priority} is a numeric value (from 1 - 100) which defines the resolution priority when attempting to find the resource with a particular @id.","title":"InProject resolver payload"},{"location":"/docs/api/kg/kg-resolvers-api.html#inaccount-resolver","text":"The scope of the resolution is the current account where the resource resides. Account resolution defines a collection of identities I to enforce ACLs. In other words:\nSchema A can import schema B using the owl:import as long as schema B is located on the same account as schema A and as long I have schemas/read permissions on the schema B project. Resource A can reference resource’s context B (inside @context) as long as resource B is located on the same account as resource A and as long as I have schemas/read permissions on the schema B project.","title":"InAccount resolver"},{"location":"/docs/api/kg/kg-resolvers-api.html#inaccount-resolver-payload","text":"{\n  \"@id\": \"{someid}\",\n  \"@type\": [\"Resolver\", \"InAccount\"],\n  \"resourceTypes\": [\"{resourceType}\", ...],\n  \"identities\": [ {_identity_}, {...} ],\n  \"priority\": {priority}\n}\nwhere…\n{resourceType} filters the @type value of the resources to be resolved. This field is optional. {priority} is a numeric value (from 1 - 100) which defines the resolution priority when attempting to find the resource with a particular @id. _identity_ the identity against which to enforce ACLs during resolution process. {someid} the @id value for this resource.","title":"InAccount resolver payload"},{"location":"/docs/api/kg/kg-resolvers-api.html#example","text":"Request curl -XPUT -H \"Content-Type: application/json\" \"https://nexus.example.com/v1/resolvers/myaccount/myproj/nxv:myresolver\" -d '{\"@type\": [\"Resolver\", \"InAccount\"], \"resourceTypes\": [\"nxv:Schema\"], \"identities\": [{\"@type\": \"GroupRef\", \"realm\": \"BBP\", \"group\": \"some-project\"} ], \"priority\": 50 }'Full source at GitHub Payload {\n  \"@type\": [\n    \"Resolver\",\n    \"InAccount\"\n  ],\n  \"resourceTypes\": [\n    \"nxv:Schema\"\n  ],\n  \"identities\": [\n    {\n      \"@type\": \"GroupRef\",\n      \"realm\": \"BBP\",\n      \"group\": \"some-project\"\n    }\n  ],\n  \"priority\": 50\n}Full source at GitHub Response {\n  \"@context\": \"https://bluebrain.github.io/nexus/contexts/resource\",\n  \"@id\": \"nxv:myresolver\",\n  \"@type\": [\n    \"nxv:Resolver\",\n    \"nxv:InAccount\"\n  ],\n  \"_self\": \"https://nexus.example.com/v1/resolvers/myaccount/myproject/nxv:myresolver\",\n  \"_constrainedBy\": \"nxs:resolver\",\n  \"_project\": \"https://nexus.example.com/v1/projects/myaccount/myproj\",\n  \"_createdAt\": \"2018-09-18T09:58:00.801Z\",\n  \"_createdBy\": \"https://nexus.example.com/v1/realms/myrealm/users/f:ad46ddd6-134e-44d6-ab70-bdf00f48dfce:someone\",\n  \"_updatedAt\": \"2018-09-18T09:58:00.801Z\",\n  \"_updatedBy\": \"https://nexus.example.com/v1/realms/myrealm/users/f:ad46ddd6-134e-44d6-ab70-bdf00f48dfce:someone\",\n  \"_rev\": 1,\n  \"_deprecated\": false\n}Full source at GitHub","title":"Example"},{"location":"/docs/api/kg/kg-resolvers-api.html#crossproject-resolver","text":"The scope of the resolution is the collections of projects P defined on the resolver. CrossProject resolution also defines a collection of identities I to enforce ACLs. In other words:\nSchema A can import schema B using the owl:import as long as schema B is located on some of the projects from the collection P and as long I have schemas/read permissions on the schema B project. Resource A can reference resource’s context B (inside @context) as long as resource B is located on some of the projects from the collection P and as long as I have schemas/read permissions on the schema B project.","title":"CrossProject resolver"},{"location":"/docs/api/kg/kg-resolvers-api.html#crossproject-resolver-payload","text":"{\n  \"@id\": \"{someid}\",\n  \"@type\": [\"Resolver\", \"CrossProject\"],\n  \"resourceTypes\": [\"{resourceType}\", ...],\n  \"projects\": [\"{project}\", ... ],\n  \"identities\": [ {_identity_}, {...} ],\n  \"priority\": 50\n}\nwhere…\n{resourceType} filters the @type value of the resources to be resolved. This field is optional. {priority} is a numeric value (from 1 - 100) which defines the resolution priority when attempting to find the resource with a particular @id. {project} the user friendly reference to the project from where the resolution process will attempt to find the @id’s. It follows the format {account}/{project} _identity_ the identity against which to enforce ACLs during resolution process. {someid} the @id value for this resource.","title":"CrossProject resolver payload"},{"location":"/docs/api/kg/kg-resolvers-api.html#example","text":"Request curl -XPUT -H \"Content-Type: application/json\" \"https://nexus.example.com/v1/resolvers/myaccount/myproj/nxv:myresolver\" -d '{\"@type\": [\"Resolver\", \"CrossProject\"], \"projects\": [\"account1/project1\", \"account1/project2\"], \"identities\": [{\"@type\": \"UserRef\", \"realm\": \"BBP\", \"sub\": \"name\"} ], \"priority\": 50 }'Full source at GitHub Payload {\n  \"@type\": [\n    \"Resolver\",\n    \"CrossProject\"\n  ],\n  \"projects\": [\n    \"account1/project1\",\n    \"account1/project2\"\n  ],\n  \"identities\": [\n    {\n      \"@type\": \"UserRef\",\n      \"realm\": \"BBP\",\n      \"sub\": \"name\"\n    }\n  ],\n  \"priority\": 50\n}Full source at GitHub Response {\n  \"@context\": \"https://bluebrain.github.io/nexus/contexts/resource\",\n  \"@id\": \"nxv:myresolver\",\n  \"@type\": [\n    \"nxv:Resolver\",\n    \"nxv:CrossProject\"\n  ],\n  \"_self\": \"https://nexus.example.com/v1/resolvers/myaccount/myproject/nxv:myresolver\",\n  \"_constrainedBy\": \"nxs:resolver\",\n  \"_project\": \"https://nexus.example.com/v1/projects/myaccount/myproj\",\n  \"_createdAt\": \"2018-09-18T09:58:00.801Z\",\n  \"_createdBy\": \"https://nexus.example.com/v1/realms/myrealm/users/f:ad46ddd6-134e-44d6-ab70-bdf00f48dfce:someone\",\n  \"_updatedAt\": \"2018-09-18T09:58:00.801Z\",\n  \"_updatedBy\": \"https://nexus.example.com/v1/realms/myrealm/users/f:ad46ddd6-134e-44d6-ab70-bdf00f48dfce:someone\",\n  \"_rev\": 1,\n  \"_deprecated\": false\n}Full source at GitHub","title":"Example"},{"location":"/docs/api/kg/kg-resolvers-api.html#create-a-cross-project-resolver-using-post","text":"POST /v1/resolver/{accountId}/{projId}\n  {...}\n ```\n\nThe json payload: \n\n- If the `@id` value is found on the payload, this @id will be used.\n- If the `@id` value is not found on the payload, an @id will be generated as follows: `base:{UUID}`. The `base` is the `prefix` defined on the resolver's project (`{projId}`).\n\n### Example\n\nRequest\n:   @@snip [resolver-cross-project.sh](../assets/resolver-cross-project.sh)\n\nPayload\n:   @@snip [resolver-cross-project.json](../assets/resolver-cross-project.json)\n\nResponse\n:   @@snip [resolver-cross-project-ref-new.json](../assets/resolver-cross-project-ref-new.json)\n\n\n## Create a (cross-project)resolver using PUT\nThis alternative endpoint to create a resolver is useful in case the json payload does not contain an `@id` but you want to specify one. The @id will be specified in the last segment of the endpoint URI.\nPUT /v1/resolvers/{accountId}/{projId}/{resolver_id} {…}\nThe json payload. Note that if the payload contains an @id different from the `{resolver_id}`, the request will fail.\n\n### Example\n\nRequest\n:   @@snip [resolver-cross-project.sh](../assets/resolver-cross-project-put.sh)\n\nPayload\n:   @@snip [resolver-cross-project.json](../assets/resolver-cross-project.json)\n\nResponse\n:   @@snip [resolver-cross-project-ref-new.json](../assets/resolver-cross-project-ref-new.json)\n\n\n## Update a (cross-project)resolver\n\nThis operation overrides the payload.\n\nIn order to ensure a client does not perform any changes to a resource without having had seen the previous revision of\nthe resolver, the last revision needs to be passed as a query parameter.\nPUT /v1/resolvers/{accountId}/{projId}/{resolver_id}?rev={previous_rev} {…}\n... where `{previous_rev}` is the last known revision number for the schema.\n\n\n### Example\n\nRequest\n:   @@snip [resolver-cross-project-update.sh](../assets/resolver-cross-project-update.sh)\n\nPayload\n:   @@snip [resolver-cross-project.json](../assets/resolver-cross-project.json)\n\nResponse\n:   @@snip [resolver-cross-project-ref-updated.json](../assets/resolver-cross-project-ref-updated.json)\n\n\n## Tag a resolver\n\nLinks a resolver revision to a specific name. \n\nTagging a resolver is considered to be an update as well.\nPUT /v1/resolvers/{accountId}/{projId}/{resolver_id}/tags?rev={previous_rev} { “tag”: “{name}”, “rev”: {rev} }\n... where \n\n- `{previous_rev}` is the last known revision number for the resolver.\n- `{name}` the name to give to the resolver at a specific revision.\n- `{rev}` the revision number to link the provided `{name}`.\n\n### Example\n\nRequest\n:   @@snip [resolver-tag.sh](../assets/resolver-tag.sh)\n\nPayload\n:   @@snip [tag.json](../assets/tag.json)\n\nResponse\n:   @@snip [resolver-cross-project-ref-tagged.json](../assets/resolver-cross-project-ref-tagged.json)\n\n\n## Deprecate a resolver\n\nLocks the resolver, so no further operations can be performed. It will also not be taken into account in the resolution process.\n\nDeprecating a resolver is considered to be an update as well.\nDELETE /v1/resolvers/{accountId}/{projId}/{resolver_id}?rev={previous_rev}\n... where `{previous_rev}` is the last known revision number for the schema.\n\n### Example\n\nRequest\n:   @@snip [resolver-deprecate.sh](../assets/resolver-deprecate.sh)\n\nResponse\n:   @@snip [resolver-ref-deprecated.json](../assets/resolver-ref-deprecated.json)\n\n\n## Fetch a resolver (current version)\nGET /v1/resolvers/{accountId}/{projId}/{resolver_id}\n### Example\n\nRequest\n:   @@snip [resolver-fetch.sh](../assets/resolver-fetch.sh)\n\nResponse\n:   @@snip [resolver-fetched.json](../assets/resolver-fetched.json)\n\n\n## Fetch a resolver (specific version)\nGET /v1/resolvers/{accountId}/{projId}/{resolver_id}?rev={rev}\n... where `{rev}` is the revision number of the resolver to be retrieved.\n\n### Example\n\nRequest\n:   @@snip [resolver-fetch-revision.sh](../assets/resolver-fetch-revision.sh)\n\nResponse\n:   @@snip [resolver-fetched.json](../assets/resolver-fetched.json)\n\n\n## Fetch a resolver (specific tag)\nGET /v1/resolvers/{accountId}/{projId}/{resolver_id}?tag={tag} ```\n… where {tag} is the tag of the resolver to be retrieved.","title":"Create a (cross-project)resolver using POST"},{"location":"/docs/api/kg/kg-resolvers-api.html#example","text":"Request curl \"https://nexus.example.com/v1/resolvers/myaccount/myproj/nxv:myresolver?tag=mytag\"Full source at GitHub Response {\n  \"@context\": [\n    \"https://bluebrain.github.io/nexus/contexts/resolver\",\n    \"https://bluebrain.github.io/nexus/contexts/resource\"\n  ],\n  \"@id\": \"nxv:myresolver\",\n\n  \"@type\": [\n    \"Resolver\",\n    \"CrossProject\"\n  ],\n  \"projects\": [\n    \"account1/project1\",\n    \"account1/project2\"\n  ],\n  \"identities\": [\n    {\n      \"@type\": \"UserRef\",\n      \"realm\": \"BBP\",\n      \"sub\": \"name\"\n    }\n  ],\n  \"priority\": 50,\n  \"_self\": \"https://nexus.example.com/v1/resolvers/myaccount/myproject/nxv:myresolver\",\n  \"_constrainedBy\": \"nxs:resolver\",\n  \"_project\": \"https://nexus.example.com/v1/projects/myaccount/myproj\",\n  \"_createdAt\": \"2018-09-18T09:58:00.801Z\",\n  \"_createdBy\": \"https://nexus.example.com/v1/realms/myrealm/users/f:ad46ddd6-134e-44d6-ab70-bdf00f48dfce:someone\",\n  \"_updatedAt\": \"2018-09-18T09:58:00.801Z\",\n  \"_updatedBy\": \"https://nexus.example.com/v1/realms/myrealm/users/f:ad46ddd6-134e-44d6-ab70-bdf00f48dfce:someone\",\n  \"_rev\": 1,\n  \"_deprecated\": false\n}Full source at GitHub","title":"Example"},{"location":"/docs/api/kg/kg-views-api.html","text":"","title":"Views"},{"location":"/docs/api/kg/kg-views-api.html#views","text":"Views are rooted in the /v1/views/{accountId}/{projId} collection.\nViews are used in the following scenarios\nTBD","title":"Views"},{"location":"/docs/api/iam-service-api.html","text":"","title":"IAM Service API"},{"location":"/docs/api/iam-service-api.html#iam-service-api","text":"TBC.","title":"IAM Service API"},{"location":"/docs/api/error-signaling.html","text":"","title":"Error Signaling"},{"location":"/docs/api/error-signaling.html#error-signaling","text":"The services makes use of the HTTP Status Codes to report the outcome of each API call. The status codes are complemented by a consistent response data model for reporting client and system level failures.\nFormat {\n  \"@context\": {\n    \"@vocab\": \"{{base}}/voc/nexus/core\"\n  },\n  \"code\": \"<a machine readable unique code>\",\n  \"message\": \"<a human readable description of the error>\"\n}Full source at GitHub Example {\n  \"@context\": {\n    \"@vocab\": \"{{base}}/voc/nexus/core\"\n  },\n  \"code\": \"IllegalFilterFormat\",\n  \"message\": \"Unable to parse 'path' as an uri\",\n  \"field\": \"DownField(filter)/DownField(path)\"\n}Full source at GitHub\nWhile the format only specifies _code and _message fields, additional fields may be presented for additional information in certain scenarios.","title":"Error Signaling"},{"location":"/docs/architecture/index.html","text":"","title":"System Architecture"},{"location":"/docs/architecture/index.html#system-architecture","text":"The architecture section is to provide a comprehensive architectural overview of the Nexus platform. It presents a number of architectural views to depict various aspects of the system. It is intended to convey the significant architectural decisions which have been made on the system.\nTBC.","title":"System Architecture"},{"location":"/docs/architecture/use-cases.html","text":"","title":"Use Cases"},{"location":"/docs/architecture/use-cases.html#use-cases","text":"","title":"Use Cases"},{"location":"/docs/architecture/components.html","text":"","title":"Components"},{"location":"/docs/architecture/components.html#components","text":"","title":"Components"},{"location":"/docs/architecture/integration.html","text":"","title":"Integration"},{"location":"/docs/architecture/integration.html#integration","text":"","title":"Integration"},{"location":"/docs/architecture/systematic-service-design.html","text":"","title":"Systematic service design"},{"location":"/docs/architecture/systematic-service-design.html#systematic-service-design","text":"TBC.","title":"Systematic service design"},{"location":"/docs/roadmap/index.html","text":"","title":"Roadmap"},{"location":"/docs/roadmap/index.html#roadmap","text":"TBC.","title":"Roadmap"},{"location":"/docs/benchmarks/index.html","text":"","title":"Benchmarks"},{"location":"/docs/benchmarks/index.html#benchmarks","text":"TBC.","title":"Benchmarks"},{"location":"/docs/benchmarks/deployment-configuration.html","text":"","title":"Deployment Configuration"},{"location":"/docs/benchmarks/deployment-configuration.html#deployment-configuration","text":"TBC.","title":"Deployment Configuration"},{"location":"/docs/benchmarks/data-volumes.html","text":"","title":"Data Volumes"},{"location":"/docs/benchmarks/data-volumes.html#data-volumes","text":"TBC.","title":"Data Volumes"},{"location":"/docs/benchmarks/scenarios.html","text":"","title":"Scenarios"},{"location":"/docs/benchmarks/scenarios.html#scenarios","text":"TBC.","title":"Scenarios"},{"location":"/docs/benchmarks/results.html","text":"","title":"Results"},{"location":"/docs/benchmarks/results.html#results","text":"TBC.","title":"Results"},{"location":"/docs/additional-info/index.html","text":"","title":"Additional Information"},{"location":"/docs/additional-info/index.html#additional-information","text":"","title":"Additional Information"},{"location":"/docs/additional-info/incremental-domain-definition.html","text":"","title":"Incremental domain definition"},{"location":"/docs/additional-info/incremental-domain-definition.html#incremental-domain-definition","text":"TBC.","title":"Incremental domain definition"},{"location":"/docs/additional-info/faq.html","text":"","title":"Frequently asked questions"},{"location":"/docs/additional-info/faq.html#frequently-asked-questions","text":"TBC.","title":"Frequently asked questions"}]}